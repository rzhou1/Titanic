{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "data_train= pd.read_csv('train.csv')\n",
    "data_test= pd.read_csv('test.csv')\n",
    "data_all= pd.concat([data_train, data_test])\n",
    "data_all = data_all.drop(['Survived'], axis=1)\n",
    "m_train = data_train.shape[0]\n",
    "m_test = data_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def Simplify_cabins(data):\n",
    "    data.Cabin = data.Cabin.fillna('N')\n",
    "    data.Cabin = data.Cabin.apply(lambda x: x[0])\n",
    "    return data\n",
    "\n",
    "def Add_Fare(data):\n",
    "    data.Fare = data.Fare.fillna(0)\n",
    "    if len(data.Fare[data.Fare==0])>0:\n",
    "        fare = np.zeros(3)\n",
    "        for i in range(0,3):\n",
    "            fare[i] = data[data['Pclass'] == i+1]['Fare'].dropna().median()\n",
    "        for i in range(0,3):\n",
    "            data.loc[(data.Fare==0)&(data['Pclass']==i+1), 'Fare'] = fare[i]\n",
    "    return data\n",
    "\n",
    "def NameSuffix(data):\n",
    "    data['LastName'] = data['Name'].apply(lambda x: x.split(',')[0])\n",
    "    data['FirstName'] = data['Name'].apply(lambda x: x.split(',')[1])\n",
    "    data['NameSuffix'] = data['FirstName'].apply(lambda x: x.split('.')[0])\n",
    "    return data\n",
    "\n",
    "def Encode_features(data):\n",
    "    e_features = ['Cabin', 'Embarked',  'Sex']\n",
    "    for feature in e_features:\n",
    "        data[feature] = data[feature].fillna(0)\n",
    "        le = preprocessing.LabelEncoder().fit(data[feature])\n",
    "        data[feature] = le.transform(data[feature])\n",
    "    return data\n",
    "\n",
    "def Feature_preprocess(data):\n",
    "    data = NameSuffix(data)\n",
    "    data = Add_Fare(data)\n",
    "    data = Simplify_cabins(data)\n",
    "    data = Encode_features(data)\n",
    "    return data\n",
    "\n",
    "data_all = Feature_preprocess(data_all)\n",
    "\n",
    "#Simplify name suffix: suffix with less than 10 counts simplified based sex, family size, and title.\n",
    "namesuffix = {' Col': 'Sld', ' Don': 'Mrs', ' Mme': 'Mrs', ' Major': 'Sld', ' Lady': 'Mrs', ' Sir': 'Mr', \n",
    "              ' Mlle': 'Miss', ' the Countess': 'Mrs', ' Jonkheer': 'Mr', ' Capt': 'Mr', ' Mr': 'Mr', ' Mrs': 'Mrs',\n",
    "             ' Miss': 'Miss', ' Master': 'Master', ' Rev': 'Rev', ' Dr': 'Dr', ' Ms': 'Miss', ' Dona': 'Mrs'}\n",
    "data_all[\"NameSuffix\"] = data_all[\"NameSuffix\"].map(namesuffix)\n",
    "\n",
    "#Add features FamilySize and IsAlone\n",
    "data_all['FamilySize'] = data_all['SibSp'] + data_all['Parch'] + 1\n",
    "\n",
    "# data_all['IsAlone'] = 0\n",
    "# data_all.loc[data_all['FamilySize']==1, 'IsAlone'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Imputer the missing values\n",
    "#Missing Fare\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "#Missing Embarked\n",
    "data_embarked = data_all[['Embarked', 'Cabin', 'Fare', 'Pclass', 'Sex', 'FamilySize']]\n",
    "\n",
    "data_embarked_exist = data_embarked.loc[(data_embarked.Embarked!=0)]\n",
    "data_embarked_null = data_embarked.loc[(data_embarked.Embarked==0)]\n",
    "x_embarked = data_embarked_exist.iloc[:, 1:]\n",
    "y_embarked = data_embarked_exist.iloc[:, 0]\n",
    "rfc_e = RandomForestClassifier(n_estimators=100)\n",
    "rfc_e.fit(x_embarked, y_embarked)\n",
    "y_embarked_hat = rfc_e.predict(data_embarked_null.iloc[:, 1:])\n",
    "data_all.loc[data_all.Embarked==0, 'Embarked'] = y_embarked_hat\n",
    "\n",
    "#Missing Age\n",
    "data_age = data_all[['Age', 'Pclass', 'Sex', 'Fare', 'FamilySize']]\n",
    "data_age_exist = data_age.loc[(data_all.Age.notnull())]\n",
    "data_age_null = data_age.loc[(data_all.Age.isnull())]\n",
    "x_age = data_age_exist.values[:, 1:]\n",
    "y_age = data_age_exist.values[:, 0]\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=100)\n",
    "rfr.fit(x_age, y_age)\n",
    "y_hat_age = rfr.predict(data_age_null.values[:,1:])\n",
    "data_all.loc[(data_all.Age.isnull()), 'Age'] = y_hat_age\n",
    "\n",
    "#Missing Cabin\n",
    "data_cabin = data_all[['Cabin', 'Pclass', 'Sex', 'Fare', 'FamilySize', 'Embarked', 'Age']]\n",
    "data_cabin_exist = data_cabin.loc[(data_cabin.Cabin!=7)]\n",
    "data_cabin_null = data_cabin.loc[(data_cabin.Cabin==7)]\n",
    "x_cabin = data_cabin_exist.iloc[:, 1:]\n",
    "y_cabin = data_cabin_exist.iloc[:, 0]\n",
    "rfc_c = RandomForestClassifier(n_estimators=100)\n",
    "rfc_c.fit(x_cabin, y_cabin)\n",
    "y_cabin_hat = rfc_c.predict(data_cabin_null.iloc[:, 1:])\n",
    "data_all.loc[data_all.Cabin==7, 'Cabin'] = y_cabin_hat\n",
    "\n",
    "data_all.loc[data_all['Cabin']==8,'Cabin'] = data_all['Cabin'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features transform\n",
    "def bin_ages(data):\n",
    "    bins=(0,5,12,20,30,55,120)\n",
    "    group_names=[\"Baby\", \"Child\", \"Teenage\", \"Young Adult\", \"Adult\", \"Senior\"]\n",
    "    categories = pd.cut(data.Age, bins, labels=group_names)\n",
    "    data.Age=categories\n",
    "    return data\n",
    "\n",
    "def bin_fares(data):\n",
    "    bins = (0,7.9,14.4,31,1000)\n",
    "    group_names=['1_q', '2_q', '3_q', '4_q']\n",
    "    categories = pd.cut(data.Fare, bins, labels=group_names)\n",
    "    data.Fare = categories\n",
    "    return data\n",
    "\n",
    "def encode_features(data):\n",
    "    e_features = ['Age', 'Fare']\n",
    "    for feature in e_features:\n",
    "        le = preprocessing.LabelEncoder().fit(data[feature])\n",
    "        data[feature] = le.transform(data[feature])\n",
    "    return data\n",
    "\n",
    "def drop_features(data):\n",
    "    return data.drop(['Ticket', 'Name', 'SibSp', 'Parch', 'FirstName', 'LastName', 'Cabin'], axis=1)\n",
    "\n",
    "def transform_feature(data):\n",
    "    data = bin_ages(data)\n",
    "    data = bin_fares(data)\n",
    "    data = drop_features(data)\n",
    "    #data = encode_features(data)\n",
    "    return data\n",
    " \n",
    "data_all = transform_feature(data_all)\n",
    "\n",
    "#Create interaction features: pclass_sex, pclass_familysize, pclass_age\n",
    "# data_all['Pclass_Sex'] = data_all['Pclass'].astype(str) + data_all['Sex'].astype(str)\n",
    "# data_all['Pclass_FamilySize'] = data_all['Pclass'].astype(str) + data_all['FamilySize'].astype(str)\n",
    "#data_all['Pclass_Age'] = data_all['Pclass'].astype(str) + data_all['Age'].astype(str)\n",
    "\n",
    "dummy_features = ['Embarked', 'NameSuffix', 'Age', 'Fare']\n",
    "\n",
    "for feature in dummy_features:\n",
    "    data_all[feature] = data_all[feature].apply(str)\n",
    "\n",
    "data_all = pd.get_dummies(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #collinearity among features: VIF\n",
    "\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# def calculate_VIF(X, threshold):\n",
    "#     cols = X.columns\n",
    "#     variables = np.arange(X.shape[1])\n",
    "#     dropped=True\n",
    "#     while dropped:\n",
    "#         dropped=False\n",
    "#         c = X[cols[variables]].values\n",
    "#         vif = [variance_inflation_factor(c, idx) for idx in range(c.shape[1])]\n",
    "#         print vif\n",
    "#         max_idx = vif.index(max(vif))\n",
    "#         if max(vif) > threshold:\n",
    "#             print (\"dropped feature:\", X[cols[variables]].columns[max_idx], 'at index', str(max_idx))\n",
    "#             variables = np.delete(variables, max_idx)\n",
    "#             dropped=True\n",
    "\n",
    "#     print \"remaining variables:\", X.columns[variables]\n",
    "#     return X[cols[variables]]\n",
    "# data_all = calculate_VIF(data_all,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting up the training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data_all.iloc[:m_train,:].drop(['PassengerId'], axis=1)\n",
    "y = data_train['Survived']\n",
    "\n",
    "data_test = data_all.iloc[m_train:,:].drop(['PassengerId'], axis=1)\n",
    "\n",
    "num_test = 0.3\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = num_test, random_state=23)\n",
    "\n",
    "accuracy_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=0,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "0.8059701492537313\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(C=1e3, penalty='l2', max_iter=100, fit_intercept=False, solver='liblinear',\n",
    "                        n_jobs=-1, random_state=0)\n",
    "lr = lr.fit(x_train, y_train)\n",
    "y_hat_lr = lr.predict(x_test)\n",
    "accuracy_dict['LogisticRegression'] = accuracy_score(y_test, y_hat_lr)\n",
    "print lr\n",
    "print accuracy_score(y_test, y_hat_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=2, min_samples_split=3,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best')\n",
      "0.8246268656716418\n"
     ]
    }
   ],
   "source": [
    "#Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='gini', max_depth=3, max_features=None, min_samples_split=3, min_samples_leaf=2,\n",
    "                            random_state=0)\n",
    "\n",
    "dt = dt.fit(x_train, y_train)\n",
    "y_hat_dt = dt.predict(x_test)\n",
    "accuracy_dict['DecisionTree'] = accuracy_score(y_test, y_hat_dt)\n",
    "print dt\n",
    "print accuracy_dict['DecisionTree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13520887 0.37782965 0.08562976 0.02239871 0.02254523 0.01395401\n",
      " 0.00594902 0.0100111  0.01266106 0.00794333 0.04319542 0.00993308\n",
      " 0.01170539 0.01261566 0.02351762 0.00135794 0.01764268 0.10066107\n",
      " 0.08222105 0.00109207 0.00192729]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=1,\n",
      "            warm_start=False)\n",
      "0.8246268656716418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import eli5\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=5, criterion='gini', max_features='sqrt', min_samples_split=2, n_jobs=-1, verbose=1)\n",
    "# parameters = {'n_estimators':[100,50], \n",
    "#               'max_features':['auto'],\n",
    "#               'criterion': ['gini'],\n",
    "#               'max_depth': [2,3,5],\n",
    "#               'min_samples_split': [2,3],\n",
    "#               'min_samples_leaf': [1,2]\n",
    "#              } \n",
    "# acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# rfc = GridSearchCV(rfc, parameters, cv=3, scoring = acc_scorer, n_jobs = -1, verbose = 1)\n",
    "rfc = rfc.fit(x_train, y_train)\n",
    "print rfc.feature_importances_\n",
    "y_hat_rfc = rfc.predict(x_test)\n",
    "accuracy_dict['RandomForest'] = accuracy_score(y_test, y_hat_rfc)\n",
    "print rfc\n",
    "print accuracy_dict['RandomForest']\n",
    "# eli5.show_weights(rfc, feature_names=list(x_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_copy = x_test\n",
    "x_test_copy['y_actual'] = y_test\n",
    "x_test_copy['y_hat_rfc'] = y_hat_rfc\n",
    "x_test_copy['miss_classified'] = x_test_copy['y_actual'] - x_test_copy['y_hat_rfc']\n",
    "x_test_copy.to_csv('x_test_rfc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12547344 0.35998483 0.03660822 0.02303087 0.02033133 0.01021307\n",
      " 0.00258779 0.0048605  0.0107459  0.00634028 0.05126883 0.00833641\n",
      " 0.00643645 0.01067546 0.02670783 0.00144168 0.01698544 0.15038597\n",
      " 0.12418472 0.00128177 0.00211921]\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=0, verbose=1, warm_start=False)\n",
      "0.8171641791044776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators=100, max_features='sqrt', max_depth=5, criterion='gini', random_state=0, n_jobs=-1,verbose=1)\n",
    "\n",
    "# params = {'n_estimators': [100,1000,10000], 'max_features': ['sqrt', 'log2'], 'max_depth': [2,3,5],\n",
    "#           'min_samples_split': [2,3,5], 'min_samples_leaf': [1,2,3]}\n",
    "\n",
    "# etc = GridSearchCV(etc, params, cv=3, n_jobs=-1, verbose=1)\n",
    "etc=etc.fit(x_train, y_train)\n",
    "#print etc.feature_importances_\n",
    "y_hat_etc=etc.predict(x_test)\n",
    "accuracy_dict['ExtraTrees'] = accuracy_score(y_test, y_hat_etc)\n",
    "\n",
    "print etc\n",
    "print accuracy_dict['ExtraTrees']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Pclass', u'Sex', u'FamilySize', u'IsAlone', u'Age_Baby', u'Age_Child',\n",
       "       u'Age_Teenage', u'Age_Young Adult', u'Age_Adult', u'Age_Senior',\n",
       "       u'Embarked_1', u'Embarked_2', u'Fare_2_q', u'Fare_3_q', u'Fare_4_q',\n",
       "       u'NameSuffix_Dr', u'NameSuffix_Master', u'NameSuffix_Miss',\n",
       "       u'NameSuffix_Mrs', u'NameSuffix_Rev', u'NameSuffix_Sld'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15915073 0.51734977 0.09985942 0.         0.01918804 0.0055361\n",
      " 0.00369718 0.00248064 0.01057819 0.02334828 0.02097423 0.02557982\n",
      " 0.02149662 0.02251591 0.         0.         0.05835859 0.\n",
      " 0.         0.         0.00988647]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.02, loss='deviance', max_depth=3,\n",
      "              max_features='auto', max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=3,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "0.8171641791044776\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.02, n_estimators=100, max_depth=3, max_features = 'auto', min_samples_split=3, loss='deviance')\n",
    "\n",
    "# params = {'loss': ['deviance'], 'learning_rate': [0.02, 0.1, 0.2], 'n_estimators': [100, 1000], 'max_depth': [2,3,5],\n",
    "#           'min_samples_split': [2,3]}\n",
    "# gbc = GridSearchCV(gbc, params, cv=3, n_jobs=-1, verbose=1)\n",
    "gbc = gbc.fit(x_train, y_train)\n",
    "#print gbc.feature_importances_\n",
    "y_hat_gbc = gbc.predict(x_test)\n",
    "accuracy_dict['GradientBoosting'] = accuracy_score(y_test, y_hat_gbc)\n",
    "print gbc\n",
    "print accuracy_dict['GradientBoosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.082 0.086 0.2   0.026 0.056 0.01  0.06  0.044 0.03  0.022 0.048 0.06\n",
      " 0.042 0.01  0.034 0.018 0.088 0.032 0.044 0.004 0.004]\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.2, n_estimators=500, random_state=0)\n",
      "0.8022388059701493\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(learning_rate=0.2, n_estimators=500, random_state=0)\n",
    "ada = ada.fit(x_train, y_train)\n",
    "#print ada.feature_importances_\n",
    "y_hat_ada = ada.predict(x_test)\n",
    "\n",
    "accuracy_dict['AdaBoost'] = accuracy_score(y_test, y_hat_ada)\n",
    "print ada\n",
    "print accuracy_dict['AdaBoost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='rbf', gamma='auto', C=1)\n",
    "\n",
    "svc = svc.fit(x_train, y_train)\n",
    "\n",
    "y_hat_svc = svc.predict(x_test)\n",
    "\n",
    "accuracy_dict['SVC'] = accuracy_score(y_test, y_hat_svc)\n",
    "print svc\n",
    "print accuracy_dict['SVC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1091703  0.08005822 0.22561863 0.         0.01892285 0.00727802\n",
      " 0.02620087 0.04512373 0.02328967 0.01746725 0.06259097 0.04512373\n",
      " 0.04512373 0.09170306 0.05822416 0.         0.08296943 0.05094614\n",
      " 0.01018923 0.         0.        ]\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.6, max_delta_step=0,\n",
      "       max_depth=2, min_child_weight=1, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0.2, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "0.8171641791044776\n"
     ]
    }
   ],
   "source": [
    "#XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgbc = xgb.XGBClassifier(base_score=0.5, max_depth=2, learning_rate=0.6, n_estimators=1000, objective = 'binary:logistic', \n",
    "                        booster = 'gbtree', reg_alpha=0.2, reg_lambda=1, random_state=0)\n",
    "# params = {'max_depth': [2,3,6], 'learning_rate': [0.05, 0.1, 0.2], 'n_estimators': [100,1000], \n",
    "#           'objective': ['binary:logistic'], 'booster': ['gbtree'], 'reg_alpha': [0.1, 0.2]}\n",
    "# xgbc = GridSearchCV(xgbc, params, n_jobs=-1)\n",
    "xgbc = xgbc.fit(x_train, y_train)\n",
    "print xgbc.feature_importances_\n",
    "y_hat_xgbc = xgbc.predict(x_test)\n",
    "accuracy_dict['XGBoost'] = accuracy_score(y_test, y_hat_xgbc)\n",
    "print xgbc\n",
    "print accuracy_score(y_test, y_hat_xgbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Pclass', u'Sex', u'FamilySize', u'IsAlone', u'Age_Baby', u'Age_Child',\n",
       "       u'Age_Teenage', u'Age_Young Adult', u'Age_Adult', u'Age_Senior',\n",
       "       u'Embarked_1', u'Embarked_2', u'Fare_2_q', u'Fare_3_q', u'Fare_4_q',\n",
       "       u'NameSuffix_Dr', u'NameSuffix_Master', u'NameSuffix_Miss',\n",
       "       u'NameSuffix_Mrs', u'NameSuffix_Rev', u'NameSuffix_Sld'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AdaBoost': 0.8022388059701493,\n",
       " 'ExtraTrees': 0.8171641791044776,\n",
       " 'GradientBoosting': 0.8171641791044776,\n",
       " 'LogisticRegression': 0.7947761194029851,\n",
       " 'RandomForest': 0.8208955223880597,\n",
       " 'SVC': 0.8246268656716418}"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a class for running the first level base models\n",
    "\n",
    "class BaseModelClassifier(object):\n",
    "    def __init__(self, clf, seed=0, params = None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "        \n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        return self.clf.fit(x,y)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "#     def feature_importance(self, x, y):\n",
    "#         print (self.clf.fit(x,y).feature_importances_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "lg = BaseModelClassifier(LogisticRegression, seed=0, params=lg_params)\n",
    "rf = BaseModelClassifier(RandomForestClassifier, seed=0, params=rf_params)\n",
    "gb = BaseModelClassifier(GradientBoostingClassifier, seed=0, params=gb_params)\n",
    "ada = BaseModelClassifier(AdaBoostClassifier, seed=0, params=ada_params)\n",
    "svm = BaseModelClassifier(SVC, seed=0, params=svm_params)\n",
    "xgb = BaseModelClassifier(XGBClassifier, seed=0, params=xgb_params)\n",
    "lgb = BaseModelClassifier(lgb.LGBMClassifier, seed=0, params=lgb_params)\n",
    "et = BaseModelClassifier(ExtraTreesClassifier, seed=0, params=et_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Out-of-fold predictions: to avoid overfitting and as stacking train data\n",
    "from sklearn.cross_validation import KFold\n",
    "from scipy import stats\n",
    "\n",
    "nfolds = 5\n",
    "kf = KFold(m_train, n_folds = nfolds, random_state=0)\n",
    "def OOFold(clf, x_train, y_train, x_test):\n",
    "    OOF_train = np.zeros((m_train,))\n",
    "    OOF_test = np.zeros((m_test,))\n",
    "    OOF_test_kf = np.zeros((nfolds, m_test))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_kf_train = x_train.iloc[train_index]\n",
    "        y_kf_train = y_train.iloc[train_index]\n",
    "        \n",
    "        x_kf_test = x_train.iloc[test_index]\n",
    "        clf.fit(x_kf_train, y_kf_train)\n",
    "        \n",
    "        OOF_train[test_index] = clf.predict(x_kf_test)\n",
    "        \n",
    "        OOF_test_kf[i, :] = clf.predict(x_test)\n",
    "        \n",
    "    OOF_test_mean = OOF_test_kf.mean(axis=0)\n",
    "    OOF_test_kf_df = pd.DataFrame(data=OOF_test_kf)\n",
    "    OOF_test_mode = OOF_test_kf_df.mode(axis=0)\n",
    "    OOF_test_mode = OOF_test_mode.values\n",
    "\n",
    "    \n",
    "    return OOF_train, OOF_test_mean, OOF_test_mode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lg': 0.819304152637486, 'svm': 0.8215488215488216, 'lgb': 0.8170594837261503, 'xgb': 0.8294051627384961, 'rf': 0.8148148148148148, 'gb': 0.8271604938271605, 'ada': 0.813692480359147, 'et': 0.8069584736251403}\n"
     ]
    }
   ],
   "source": [
    "models = [lg, rf, gb, ada, svm, xgb, lgb, et]\n",
    "\n",
    "def get_oof_prediction(models):\n",
    "    nrows_train = data_train.shape[0]\n",
    "    nrows_test = data_test.shape[0]\n",
    "    oof_train_pred = np.zeros((nrows_train, len(models)))\n",
    "    oof_test_mean_pred = np.zeros((nrows_test, len(models)))\n",
    "    oof_test_mode_pred = np.zeros((nrows_test, len(models)))\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        oof_train_pred[:, i], oof_test_mean_pred[:, i], oof_test_mode_pred[:, i] = OOFold(model, x, y, data_test)\n",
    "    oof_train_pred = pd.DataFrame(data=oof_train_pred)\n",
    "    oof_test_mean_pred = pd.DataFrame(data=oof_test_mean_pred)\n",
    "    oof_test_mode_pred = pd.DataFrame(data=oof_test_mode_pred)\n",
    "    return oof_train_pred, oof_test_mean_pred, oof_test_mode_pred\n",
    "        \n",
    "oof_train, oof_test_mean, oof_test_mode = get_oof_prediction(models)\n",
    "\n",
    "columns = {0: 'lg', 1: 'rf', 2: 'gb', 3: 'ada', 4: 'svm', 5: 'xgb', 6: 'lgb', 7: 'et'}\n",
    "oof_train = oof_train.rename(columns=columns)\n",
    "oof_test_mean = oof_test_mean.rename(columns=columns)\n",
    "oof_test_mode = oof_test_mode.rename(columns=columns)\n",
    "\n",
    "accuracy_base_oof = {}\n",
    "for col in oof_train.columns.tolist():\n",
    "    accuracy_base_oof[col]= accuracy_score(y, oof_train[col])\n",
    "\n",
    "print accuracy_base_oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~rzhou1/0 or inside your plot.ly account where it is named 'oof_train_corr'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~rzhou1/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.tools.set_credentials_file(username='rzhou1', api_key='rGZu418WMZgckbLCiEtm')\n",
    "\n",
    "data = [go.Heatmap(z=oof_train.astype(float).corr().values,\n",
    "                  y=oof_train.columns.values,\n",
    "                  x=oof_train.columns.values,\n",
    "                  showscale=True)]\n",
    "py.iplot(data, filename='oof_train_corr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 283 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 533 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 883 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1296 out of 1296 | elapsed:  3.1min finished\n"
     ]
    }
   ],
   "source": [
    "#LightGBM\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lgb = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary',random_state=0)\n",
    "\n",
    "params = {'learning_rate': [0.05, 0.1, 0.2], 'n_estimators': [100,500,1000,5000], 'num_leaves': [5,10,31], \n",
    "          'max_depth': [2,3,5,8], 'min_child_samples': [3,5,8]}\n",
    "lgb = GridSearchCV(lgb, params, cv=3, verbose=1, n_jobs=-1)\n",
    "lgb.fit(oof_train, y)\n",
    "y_stack_hat_lgb = lgb.predict(oof_test_mean)\n",
    "\n",
    "y_stack_hat_lgb = pd.DataFrame(data=y_stack_hat_lgb)\n",
    "y_stack_hat_lgb = pd.DataFrame(data={'PassengerId': data_all.iloc[m_train:, 0], 'Survived': y_stack_hat_lgb.iloc[:,0]})\n",
    "y_stack_hat_lgb.to_csv('sub_stack1_sel6_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stacking packages\n",
    "\n",
    "# https://github.com/vecxoz/vecstack - a Python package for stacking \n",
    "# https://github.com/rasbt/mlxtend - Machine learning extension package with stacking \n",
    "# https://github.com/mpearmain/gestalt - Data pipeline package with stacking features \n",
    "# https://github.com/ndemir/stacking - Python helper functions and examples."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
